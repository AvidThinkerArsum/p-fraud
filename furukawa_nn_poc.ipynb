{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b385a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 19:51:30.470009: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ea3aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This turns all the axes white in all the matplotlib plots. Comment this out if you dont want that\n",
    "COLOR = 'white'\n",
    "matplotlib.rcParams['text.color'] = COLOR\n",
    "matplotlib.rcParams['axes.labelcolor'] = COLOR\n",
    "matplotlib.rcParams['xtick.color'] = COLOR\n",
    "matplotlib.rcParams['ytick.color'] = COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a983c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>V330</th>\n",
       "      <th>V331</th>\n",
       "      <th>V332</th>\n",
       "      <th>V333</th>\n",
       "      <th>V334</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 394 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5  ... V330  V331  V332  V333  V334 V335  \\\n",
       "0    NaN  150.0    discover  142.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       "1  404.0  150.0  mastercard  102.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       "2  490.0  150.0        visa  166.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       "3  567.0  150.0  mastercard  117.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
       "4  514.0  150.0  mastercard  102.0  ...  0.0   0.0   0.0   0.0   0.0  0.0   \n",
       "\n",
       "  V336  V337  V338  V339  \n",
       "0  NaN   NaN   NaN   NaN  \n",
       "1  NaN   NaN   NaN   NaN  \n",
       "2  NaN   NaN   NaN   NaN  \n",
       "3  NaN   NaN   NaN   NaN  \n",
       "4  0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 394 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transaction = pd.read_csv('./datasets/ieee-fraud-detection/train_transaction.csv')\n",
    "\n",
    "df_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa26b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that are used, isFraud is the target\n",
    "features = ['isFraud', 'TransactionDT',\n",
    "            'TransactionAmt','ProductCD', 'P_emaildomain','R_emaildomain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec0bb142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>R_emaildomain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>outlook.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   isFraud  TransactionDT  TransactionAmt ProductCD P_emaildomain  \\\n",
       "0        0          86400            68.5         W           NaN   \n",
       "1        0          86401            29.0         W     gmail.com   \n",
       "2        0          86469            59.0         W   outlook.com   \n",
       "3        0          86499            50.0         W     yahoo.com   \n",
       "4        0          86506            50.0         H     gmail.com   \n",
       "\n",
       "  R_emaildomain  \n",
       "0           NaN  \n",
       "1           NaN  \n",
       "2           NaN  \n",
       "3           NaN  \n",
       "4           NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_transaction[features]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17eecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'isFraud'\n",
    "# Categorical features\n",
    "cat = ['TransactionDT','ProductCD', 'P_emaildomain','R_emaildomain']\n",
    "# Numeric features\n",
    "num = ['TransactionAmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bef64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with missing features\n",
    "df = df.dropna()\n",
    "y = df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1587519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat = df.filter(items = cat).values\n",
    "x_num = df.filter(items = num).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce1ad568",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_X = LabelEncoder()\n",
    "# Label encode every categorical column\n",
    "for i in range(len(cat)): \n",
    "    x_cat[:, i] = labelencoder_X.fit_transform(x_cat[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b68bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input vector X, the training data\n",
    "X = np.concatenate((x_cat, x_num), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3c9d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7fdfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype('float32')\n",
    "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "X_val = np.asarray(X_val).astype('float32')\n",
    "y_val = np.asarray(y_val).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a7c757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80784, 5), (80784, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e118db46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 19:51:59.789410: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2525/2525 [==============================] - 2s 584us/step - loss: 124.3427 - accuracy: 0.8406\n",
      "Epoch 2/3\n",
      "2525/2525 [==============================] - 1s 578us/step - loss: 37.3123 - accuracy: 0.8408\n",
      "Epoch 3/3\n",
      "2525/2525 [==============================] - 1s 581us/step - loss: 14.3877 - accuracy: 0.8438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84e8ea3d30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = tf.keras.Sequential() # initializing the model\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu)) # first dense layer with 128 neurons with rectified linear unit for a spectrum of values.\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu)) # second layer\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)) # final layer with sigmoid for binary classification\n",
    "model.compile(optimizer='adam', # optomizing weight with adam using stochastic gradient descent\n",
    "              loss='binary_crossentropy', # evaluate performance of model with binary_crossentropy as output is binary\n",
    "              metrics=['accuracy']) # gives out accuracy of model\n",
    "model.fit(X_train, y_train, epochs=3) # pass training data 3 times through model and fit\n",
    "\n",
    "# loss is on training data, lower loss is good but might overfit\n",
    "# accuracy is on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abdab03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632/632 [==============================] - 0s 357us/step - loss: 7.1495 - accuracy: 0.9173\n",
      "7.1495466232299805 0.9173144698143005\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(val_loss, val_acc)\n",
    "# accuracy is on validation data - performance in wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b988f677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2525/2525 [==============================] - 2s 682us/step - loss: 19.5896 - accuracy: 0.8519\n",
      "Epoch 2/3\n",
      "2525/2525 [==============================] - 2s 683us/step - loss: 0.5507 - accuracy: 0.9071\n",
      "Epoch 3/3\n",
      "2525/2525 [==============================] - 2s 692us/step - loss: 0.2901 - accuracy: 0.9162\n",
      "632/632 [==============================] - 0s 384us/step - loss: 0.2854 - accuracy: 0.9172\n",
      "0.2854485511779785 0.9171659350395203\n"
     ]
    }
   ],
   "source": [
    "# Adding extra dense layer decreases loss and increases accuracy\n",
    "\n",
    "model = tf.keras.Sequential() \n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  \n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu)) \n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)) \n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy']) \n",
    "model.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f27a7c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2521/2521 [==============================] - 2s 582us/step - loss: 60.4412 - accuracy: 0.8424\n",
      "Epoch 2/3\n",
      "2521/2521 [==============================] - 1s 580us/step - loss: 21.7096 - accuracy: 0.8429\n",
      "Epoch 3/3\n",
      "2521/2521 [==============================] - 1s 580us/step - loss: 10.1021 - accuracy: 0.8429\n",
      "631/631 [==============================] - 0s 358us/step - loss: 6.7085 - accuracy: 0.9204\n",
      "6.708495140075684 0.9204027056694031\n"
     ]
    }
   ],
   "source": [
    "# Adding more features to the model\n",
    "features = ['isFraud', 'TransactionDT',\n",
    "            'TransactionAmt','ProductCD', 'P_emaildomain','R_emaildomain', 'card4']\n",
    "\n",
    "df = df_transaction[features]\n",
    "df.head()\n",
    "\n",
    "target = 'isFraud'\n",
    "cat = ['TransactionDT','ProductCD', 'P_emaildomain','R_emaildomain', 'card4']\n",
    "num = ['TransactionAmt']\n",
    "\n",
    "df = df.dropna()\n",
    "y = df[target].values\n",
    "\n",
    "x_cat = df.filter(items = cat).values \n",
    "x_num = df.filter(items = num).values\n",
    "\n",
    "labelencoder_X = LabelEncoder()\n",
    "for i in range(len(cat)): \n",
    "    x_cat[:, i] = labelencoder_X.fit_transform(x_cat[:, i])\n",
    "    \n",
    "X = np.concatenate((x_cat, x_num), axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)\n",
    "\n",
    "X_train = np.asarray(X_train).astype('float32') \n",
    "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "X_val = np.asarray(X_val).astype('float32')\n",
    "y_val = np.asarray(y_val).astype('float32').reshape((-1,1))\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "model = tf.keras.Sequential() \n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  \n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)) \n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy']) \n",
    "model.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(val_loss, val_acc)\n",
    "\n",
    "# For same number of layers, adding the extra feature of card4 does not really enhance the predictive prowess of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a61e326",
   "metadata": {},
   "source": [
    "#Feature Importance\n",
    "We are going to determine which feature is the most important for predicting the target feature \"isFraud\"\n",
    "\n",
    "Methology:\n",
    "Use the get_weights method of the Dense layer object. This method returns a list of two numpy arrays, the first of which contains the weight values and the second contains the bias values for the layer.\n",
    "\n",
    "Example code:\n",
    "Methology below compares each feature against \"isFraud\" target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "920878ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance Scores:\n",
      "TransactionDT: 0.007\n",
      "TransactionAmt: 0.007\n",
      "ProductCD: 0.005\n",
      "P_emaildomain: 0.006\n",
      "R_emaildomain: 0.007\n",
      "card4: 0.007\n"
     ]
    }
   ],
   "source": [
    "# This is for the first hidden layer. \n",
    "\n",
    "#Get the weights for the first hidden layer\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "# Calculate the feature importance scores as the absolute sum of the weights for each feature\n",
    "importance_scores = np.abs(weights).sum(axis=0)\n",
    "\n",
    "# Normalize the scores to sum to 1\n",
    "importance_scores = importance_scores / importance_scores.sum()\n",
    "\n",
    "# Print the importance scores for each feature\n",
    "print('Feature Importance Scores:')\n",
    "for i, feature in enumerate(features[1:]):\n",
    "    print(f'{feature}: {importance_scores[i]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7659e70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_0 Feature Importance Scores:\n",
      "TransactionDT: 0.007\n",
      "TransactionAmt: 0.007\n",
      "ProductCD: 0.005\n",
      "P_emaildomain: 0.006\n",
      "R_emaildomain: 0.007\n",
      "card4: 0.007\n",
      "layer_1 Feature Importance Scores:\n",
      "TransactionDT: 0.007\n",
      "TransactionAmt: 0.008\n",
      "ProductCD: 0.008\n",
      "P_emaildomain: 0.008\n",
      "R_emaildomain: 0.008\n",
      "card4: 0.008\n",
      "layer_2 Feature Importance Scores:\n",
      "TransactionDT: 1.000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Feature Importance Scores:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimportance_scores[i]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "#The methology below implements feature importance for each layer\n",
    "\n",
    "# Define a function to calculate and normalize feature importance scores for a given layer\n",
    "def get_feature_importance(layer):\n",
    "    # Get the weights for the layer\n",
    "    weights = layer.get_weights()[0]\n",
    "\n",
    "    # Calculate the feature importance scores as the absolute sum of the weights for each feature\n",
    "    importance_scores = np.abs(weights).sum(axis=0)\n",
    "\n",
    "    # Normalize the scores to sum to 1\n",
    "    importance_scores = importance_scores / importance_scores.sum()\n",
    "\n",
    "    return importance_scores\n",
    "\n",
    "# Calculate the feature importance scores for each layer\n",
    "layer_importance = {}\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if isinstance(layer, tf.keras.layers.Dense):\n",
    "        layer_importance[f'layer_{i}'] = get_feature_importance(layer)\n",
    "\n",
    "# Print the importance scores for each layer and feature\n",
    "for layer_name, importance_scores in layer_importance.items():\n",
    "    print(f'{layer_name} Feature Importance Scores:')\n",
    "    for i, feature in enumerate(features[1:]):\n",
    "        print(f'{feature}: {importance_scores[i]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65447a4a",
   "metadata": {},
   "source": [
    "#Undersampling\n",
    "Implementing NearMiss Undersampling technique\n",
    "\n",
    "This method selects the majority class examples that are closest to the minority class examples, based on a distance metric. This can help focus on the most informative majority class examples and reduce the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "689d14d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# Instantiate the NearMiss object\n",
    "nm = NearMiss(version=2, sampling_strategy='majority', n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58d21dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply undersampling to the original data\n",
    "X_train_resampled, y_train_resampled = nm.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab925148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (80656, 6) (80656, 1)\n",
      "Resampled data shape: (13500, 6) (13500, 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert data to numpy arrays and preprocess\n",
    "X_train_resampled = np.asarray(X_train_resampled).astype('float32')\n",
    "y_train_resampled = np.asarray(y_train_resampled).astype('float32').reshape((-1, 1))\n",
    "X_val = np.asarray(X_val).astype('float32')\n",
    "y_val = np.asarray(y_val).astype('float32').reshape((-1, 1))\n",
    "X_test = np.asarray(X_test).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32').reshape((-1, 1))\n",
    "\n",
    "# Print the shape of the resampled data\n",
    "print('Original data shape:', X_train.shape, y_train.shape)\n",
    "print('Resampled data shape:', X_train_resampled.shape, y_train_resampled.shape)\n",
    "\n",
    "#Save the undersampled data for NearMiss\n",
    "X_train_nm = X_train_resampled\n",
    "y_train_nm = y_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a331d6c",
   "metadata": {},
   "source": [
    "#Implement Tomek Links Undersampling\n",
    "\n",
    "This method involves removing samples that are classified as borderline cases, where there is a very small distance between samples of the minority class and majority class. This can help in removing noisy or ambiguous data and improve classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "145b245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (80656, 6) (80656, 1)\n",
      "Resampled data shape: (79423, 6) (79423, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# Instantiate the TomekLinks object\n",
    "tl = TomekLinks()\n",
    "\n",
    "# Apply undersampling to the original data\n",
    "X_train_resampled, y_train_resampled = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "# Convert data to numpy arrays and preprocess\n",
    "X_train_resampled = np.asarray(X_train_resampled).astype('float32')\n",
    "y_train_resampled = np.asarray(y_train_resampled).astype('float32').reshape((-1, 1))\n",
    "X_val = np.asarray(X_val).astype('float32')\n",
    "y_val = np.asarray(y_val).astype('float32').reshape((-1, 1))\n",
    "X_test = np.asarray(X_test).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32').reshape((-1, 1))\n",
    "\n",
    "# Print the shape of the resampled data\n",
    "print('Original data shape:', X_train.shape, y_train.shape)\n",
    "print('Resampled data shape:', X_train_resampled.shape, y_train_resampled.shape)\n",
    "\n",
    "#Save the undersampled data for Tomek Links\n",
    "X_train_tl = X_train_resampled\n",
    "y_train_tl = y_train_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "350fb3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "422/422 [==============================] - 1s 1ms/step - loss: 111.0645 - accuracy: 0.4999 - val_loss: 121.0252 - val_accuracy: 0.0796\n",
      "Epoch 2/3\n",
      "422/422 [==============================] - 1s 1ms/step - loss: 26.9460 - accuracy: 0.5001 - val_loss: 0.5632 - val_accuracy: 0.8951\n",
      "Epoch 3/3\n",
      "422/422 [==============================] - 1s 1ms/step - loss: 5.0641 - accuracy: 0.4988 - val_loss: 1.0877 - val_accuracy: 0.8898\n",
      "788/788 [==============================] - 0s 380us/step - loss: 1.1114 - accuracy: 0.8828\n",
      "Resampled data: 1.1114143133163452 0.8828057050704956\n"
     ]
    }
   ],
   "source": [
    "#Passing NearMiss sorted data to the model\n",
    "\n",
    "model = tf.keras.Sequential() \n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu)) \n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)) \n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy']) \n",
    "model.fit(X_train_nm, y_train_nm, epochs=3, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Resampled data:', test_loss, test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dc963d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2482/2482 [==============================] - 2s 688us/step - loss: 19.9460 - accuracy: 0.8569\n",
      "Epoch 2/3\n",
      "2482/2482 [==============================] - 2s 681us/step - loss: 0.3362 - accuracy: 0.9133\n",
      "Epoch 3/3\n",
      "2482/2482 [==============================] - 2s 681us/step - loss: 0.2915 - accuracy: 0.9150\n",
      "788/788 [==============================] - 0s 387us/step - loss: 0.2964 - accuracy: 0.9128\n",
      "Resampled data: 0.29636842012405396 0.9127588868141174\n"
     ]
    }
   ],
   "source": [
    "#Passing Tomek Links sorted data to the model\n",
    "\n",
    "model = tf.keras.Sequential() \n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  \n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu)) \n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)) \n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy']) \n",
    "model.fit(X_train_tl, y_train_tl, epochs=3)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Resampled data:', test_loss, test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd2ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

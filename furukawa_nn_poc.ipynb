{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b385a5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mn/_5nfz9ts7nb2bwvk93j7h3800000gn/T/ipykernel_7868/216284436.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This turns all the axes white in all the matplotlib plots. Comment this out if you dont want that\n",
    "COLOR = 'white'\n",
    "matplotlib.rcParams['text.color'] = COLOR\n",
    "matplotlib.rcParams['axes.labelcolor'] = COLOR\n",
    "matplotlib.rcParams['xtick.color'] = COLOR\n",
    "matplotlib.rcParams['ytick.color'] = COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a983c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction = pd.read_csv('../datasets/ieee-fraud-detection/train_transaction.csv')\n",
    "\n",
    "df_transaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa26b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that are used, isFraud is the target\n",
    "features = ['isFraud', 'TransactionDT',\n",
    "            'TransactionAmt','ProductCD', 'P_emaildomain','R_emaildomain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0bb142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_transaction[features]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17eecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'isFraud'\n",
    "# Categorical features\n",
    "cat = ['TransactionDT','ProductCD', 'P_emaildomain','R_emaildomain']\n",
    "# Numeric features\n",
    "num = ['TransactionAmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with missing features\n",
    "df = df.dropna()\n",
    "y = df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat = df.filter(items = cat).values\n",
    "x_num = df.filter(items = num).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ad568",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_X = LabelEncoder()\n",
    "# Label encode every categorical column\n",
    "for i in range(len(cat)): \n",
    "    x_cat[:, i] = labelencoder_X.fit_transform(x_cat[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b68bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input vector X, the training data\n",
    "X = np.concatenate((x_cat, x_num), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fdfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype('float32')\n",
    "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "X_val = np.asarray(X_val).astype('float32')\n",
    "y_val = np.asarray(y_val).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e118db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = tf.keras.Sequential() # initializing the model\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu)) # first dense layer with 128 neurons with rectified linear unit for a spectrum of values.\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu)) # second layer\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)) # final layer with sigmoid for binary classification\n",
    "model.compile(optimizer='adam', # optomizing weight with adam using stochastic gradient descent\n",
    "              loss='binary_crossentropy', # evaluate performance of model with binary_crossentropy as output is binary\n",
    "              metrics=['accuracy']) # gives out accuracy of model\n",
    "model.fit(X_train, y_train, epochs=3) # pass training data 3 times through model and fit\n",
    "\n",
    "# loss is on training data, lower loss is good but might overfit\n",
    "# accuracy is on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdab03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(val_loss, val_acc)\n",
    "# accuracy is on validation data - performance in wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding extra dense layer decreases loss and increases accuracy\n",
    "\n",
    "model = tf.keras.Sequential() \n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  \n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu)) \n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)) \n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy']) \n",
    "model.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding more features to the model\n",
    "features = ['isFraud', 'TransactionDT',\n",
    "            'TransactionAmt','ProductCD', 'P_emaildomain','R_emaildomain', 'card4']\n",
    "\n",
    "df = df_transaction[features]\n",
    "df.head()\n",
    "\n",
    "target = 'isFraud'\n",
    "cat = ['TransactionDT','ProductCD', 'P_emaildomain','R_emaildomain', 'card4']\n",
    "num = ['TransactionAmt']\n",
    "\n",
    "df = df.dropna()\n",
    "y = df[target].values\n",
    "\n",
    "x_cat = df.filter(items = cat).values \n",
    "x_num = df.filter(items = num).values\n",
    "\n",
    "labelencoder_X = LabelEncoder()\n",
    "for i in range(len(cat)): \n",
    "    x_cat[:, i] = labelencoder_X.fit_transform(x_cat[:, i])\n",
    "    \n",
    "X = np.concatenate((x_cat, x_num), axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)\n",
    "\n",
    "X_train = np.asarray(X_train).astype('float32') \n",
    "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "X_val = np.asarray(X_val).astype('float32')\n",
    "y_val = np.asarray(y_val).astype('float32').reshape((-1,1))\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "model = tf.keras.Sequential() \n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))  \n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)) \n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy']) \n",
    "model.fit(X_train, y_train, epochs=3)\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(val_loss, val_acc)\n",
    "\n",
    "# For same number of layers, adding the extra feature of card4 does not really enhance the predictive prowess of our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a61e326",
   "metadata": {},
   "source": [
    "#Feature Importance\n",
    "We are going to determine which feature is the most important for predicting the target feature \"isFraud\"\n",
    "\n",
    "Methology:\n",
    "Use the get_weights method of the Dense layer object. This method returns a list of two numpy arrays, the first of which contains the weight values and the second contains the bias values for the layer.\n",
    "\n",
    "Example code:\n",
    "Methology below compares each feature against \"isFraud\" target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920878ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for the first hidden layer. \n",
    "\n",
    "#Get the weights for the first hidden layer\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "# Calculate the feature importance scores as the absolute sum of the weights for each feature\n",
    "importance_scores = np.abs(weights).sum(axis=0)\n",
    "\n",
    "# Normalize the scores to sum to 1\n",
    "importance_scores = importance_scores / importance_scores.sum()\n",
    "\n",
    "# Print the importance scores for each feature\n",
    "print('Feature Importance Scores:')\n",
    "for i, feature in enumerate(features[1:]):\n",
    "    print(f'{feature}: {importance_scores[i]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7659e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The methology below implements feature importance for each layer\n",
    "\n",
    "# Define a function to calculate and normalize feature importance scores for a given layer\n",
    "def get_feature_importance(layer):\n",
    "    # Get the weights for the layer\n",
    "    weights = layer.get_weights()[0]\n",
    "\n",
    "    # Calculate the feature importance scores as the absolute sum of the weights for each feature\n",
    "    importance_scores = np.abs(weights).sum(axis=0)\n",
    "\n",
    "    # Normalize the scores to sum to 1\n",
    "    importance_scores = importance_scores / importance_scores.sum()\n",
    "\n",
    "    return importance_scores\n",
    "\n",
    "# Calculate the feature importance scores for each layer\n",
    "layer_importance = {}\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if isinstance(layer, tf.keras.layers.Dense):\n",
    "        layer_importance[f'layer_{i}'] = get_feature_importance(layer)\n",
    "\n",
    "# Print the importance scores for each layer and feature\n",
    "for layer_name, importance_scores in layer_importance.items():\n",
    "    print(f'{layer_name} Feature Importance Scores:')\n",
    "    for i, feature in enumerate(features[1:]):\n",
    "        print(f'{feature}: {importance_scores[i]:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
